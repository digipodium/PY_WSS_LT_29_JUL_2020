{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "9ad268fd485861c22c359044ed3ddbe8fb481338325f6875a93294b1083849ba"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = requests.get(\"https://blog.scrapinghub.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(webpage.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "headings = soup.find_all('a')\n",
    "print(len(headings))\n",
    "# incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTHE SCRAPINGHUB BLOG\nTurn Web Content Into Useful Data\n\n\n\n\nA Practical Guide to Web Data QA Part V: Broad Crawls\nSeptember 30, 2020 \nIvan Ivanov\n0 Comment\n first part\n second part\n third part\nfourth part\nRead More\n\n\n\nAnnouncing The Web Data Extraction Summit 2020\nSeptember 24, 2020 \nHimanshi Bhatt\n0 Comment\nRead More\n\n\n\nNews & Article Data Extraction: Open Source vs Closed Source Solutions\nSeptember 10, 2020 \nAttila Tóth\n0 Comment\nRead More\n\n\n\nA PRACTICAL GUIDE TO WEB DATA QA PART IV: COMPLEMENTING SEMI-AUTOMATED TECHNIQUES\nSeptember 03, 2020 \nIvan Ivanov and Warley Ferreira Lopes\n0 Comment\nfirst part\nthe second\nthird part\nRead More\n\n\n\nReal Estate: Use Web Data Extraction to Make Smarter Decisions\nAugust 27, 2020 \nAttila Tóth\n0 Comment\nRead More\n\n\n\nScrapy Cloud Secrets: Hub Crawl Frontier and How To Use It\nAugust 06, 2020 \nJúlio César Batista\n0 Comment\nRead More\n\n\n\nBlog Comments API (BETA): Extract Blog Comment DATA At Scale\nJuly 30, 2020 \nJohn Campbell\n0 Comment\nBlog Comments API\nRead More\n\n\n\nYour Price Intelligence Questions Answered\nJuly 28, 2020 \nHimanshi Bhatt\n0 Comment\nRead More\n\n\n\nData Center Proxies vs. Residential Proxies\nJuly 21, 2020 \nAttila Tóth\n0 Comment\nRead More\n\n\n\nHow to Get High Success Rates With Proxies: 3 Steps to Scale Up\nJuly 14, 2020 \nAttila Tóth\n0 Comment\nrotating proxy solution\nRead More\nOLDER POST\n\nScrapy Cloud Secrets: Hub Crawl Frontier And How To Use It\n\n\n\n\nLearn how to configure and utilize proxies with Python Requests module\nAn Introduction to XPath: How to Get Started\nHandling JavaScript in Scrapy with Splash\nHow to Build your own Price Monitoring Tool\nHow to use a proxy in Puppeteer\nScrapinghub (94)\nScrapy (45)\nWeb Scraping (37)\nReleases (29)\nOpen source (20)\nscrapy (19)\ndata extraction (17)\nCrawlera (16)\nweb crawling (16)\nScrapy Cloud (15)\nWeb Data (11)\nAutoExtract (10)\nProxies (10)\nScrapy Tips from the Pros (9)\nopen source (9)\nSeptember 2020 (4)\nAugust 2020 (2)\nJuly 2020 (6)\nJune 2020 (3)\nMay 2020 (1)\nApril 2020 (3)\nMarch 2020 (6)\nFebruary 2020 (4)\nJanuary 2020 (3)\nDecember 2019 (3)\nNovember 2019 (3)\nOctober 2019 (4)\nSeptember 2019 (4)\nAugust 2019 (3)\nJuly 2019 (2)\n"
     ]
    }
   ],
   "source": [
    "for i in headings:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14\nTurn Web Content Into Useful Data\nA Practical Guide to Web Data QA Part V: Broad Crawls\nAnnouncing The Web Data Extraction Summit 2020\nNews & Article Data Extraction: Open Source vs Closed Source Solutions\nA PRACTICAL GUIDE TO WEB DATA QA PART IV: COMPLEMENTING SEMI-AUTOMATED TECHNIQUES\nReal Estate: Use Web Data Extraction to Make Smarter Decisions\nScrapy Cloud Secrets: Hub Crawl Frontier and How To Use It\nBlog Comments API (BETA): Extract Blog Comment DATA At Scale\nA reliable and scalable way to tap into blog comment  driven insights\nYour Price Intelligence Questions Answered\nWhat is Price Intelligence?\nData Center Proxies vs. Residential Proxies\nHow to Get High Success Rates With Proxies: 3 Steps to Scale Up\nStory of the Month\n"
     ]
    }
   ],
   "source": [
    "headings = soup.find_all('h2')\n",
    "print(len(headings))\n",
    "for item in headings:\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\nA Practical Guide to Web Data QA Part V: Broad Crawls\nAnnouncing The Web Data Extraction Summit 2020\nNews & Article Data Extraction: Open Source vs Closed Source Solutions\nA PRACTICAL GUIDE TO WEB DATA QA PART IV: COMPLEMENTING SEMI-AUTOMATED TECHNIQUES\nReal Estate: Use Web Data Extraction to Make Smarter Decisions\nScrapy Cloud Secrets: Hub Crawl Frontier and How To Use It\nBlog Comments API (BETA): Extract Blog Comment DATA At Scale\nYour Price Intelligence Questions Answered\nData Center Proxies vs. Residential Proxies\nHow to Get High Success Rates With Proxies: 3 Steps to Scale Up\n"
     ]
    }
   ],
   "source": [
    "headings = soup.find_all('div',attrs={'class':'post-header'})\n",
    "print(len(headings))\n",
    "for item in headings:\n",
    "    print(item.find('h2').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all ten dates with same logic from the webpage"
   ]
  }
 ]
}